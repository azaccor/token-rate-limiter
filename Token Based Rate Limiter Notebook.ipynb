{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6975529c-6d18-4c7c-9985-4ec0cb787b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Token-Based Rate Limiting on Databricks Powered by Lakebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f095535-968f-454a-9cc4-fe00cc2c69bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install psycopg2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f01fc30-e56c-4693-9202-3a314d3eb53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Env Vars and Sample Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cad14b-2291-47ca-9160-264af0c4299e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '<FILL_IN>' # or whatever FM API key\n",
    "os.environ['DATABRICKS_TOKEN'] = '<FILL_IN>'\n",
    "os.environ['POSTGRES_HOST'] = '<FILL_IN>'\n",
    "os.environ['POSTGRES_DBNAME'] = 'databricks_postgres' # or '<FILL_IN>'\n",
    "os.environ['POSTGRES_USER'] = '<FILL_IN>'\n",
    "os.environ['POSTGRES_SSLMODE'] = '<FILL_IN>'\n",
    "os.environ['POSTGRES_PORT'] = 5432 # or '<FILL_IN>'\n",
    "os.environ['POSTGRES_PASSWORD'] = '<FILL_IN>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289a186e-546f-4b25-bbd6-a972545d27f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the loaded model with proper input\n",
    "test_input = {\n",
    "    \"user_name\": \"test.user@databricks.com\",\n",
    "    \"model\": \"gpt-4.1-2025-04-14\",\n",
    "    \"messages\": '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}',\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf518181-bf3b-49f5-9858-e6bea0ba7167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and Seed Required Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14784066-d322-46a8-8067-8884bd274e54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create token_usage table for tracking all API calls\n",
    "CREATE TABLE IF NOT EXISTS token_usage (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "    user_name VARCHAR(255) NOT NULL,\n",
    "    model_name VARCHAR(100) NOT NULL,\n",
    "    prompt_tokens INTEGER NOT NULL,\n",
    "    completion_tokens INTEGER NOT NULL,\n",
    "    total_tokens INTEGER NOT NULL,\n",
    "    request_timestamp TIMESTAMP NOT NULL,\n",
    "    request_id VARCHAR(255),\n",
    "    response_content STRING,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Create user_token_limits table for managing quotas\n",
    "CREATE TABLE IF NOT EXISTS user_token_limits (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "    user_name VARCHAR(255) NOT NULL,\n",
    "    model_name VARCHAR(100) NOT NULL,\n",
    "    token_limit INTEGER NOT NULL,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Insert sample user limit\n",
    "INSERT INTO user_token_limits (user_name, model_name, token_limit) \n",
    "VALUES ('test.user@databricks.com', 'gpt-4.1-2025-04-14', 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a495a26-0995-48e5-baf4-b873ff5ae1d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define our Token Based Rate Limiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f16f82-6c5d-49b1-ba80-425f092b82b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "import mlflow.models\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class TokenLimitedGatewayModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Initialize database connection and endpoint URL\"\"\"\n",
    "        self.conn = psycopg2.connect(\n",
    "            host=os.environ['POSTGRES_HOST'],\n",
    "            dbname=os.environ['POSTGRES_DBNAME'],\n",
    "            user=os.environ['POSTGRES_USER'],\n",
    "            password=os.environ['POSTGRES_PASSWORD'],\n",
    "            port=int(os.environ.get('POSTGRES_PORT', 5432)),\n",
    "            sslmode=os.environ.get('POSTGRES_SSLMODE', 'require')\n",
    "        )\n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "        # FM endpoint\n",
    "        self.fm_endpoint = \"<populate_fm_endpoint>\"\n",
    "        \n",
    "        # Get API token from environment if needed\n",
    "        self.api_token = os.environ.get('DATABRICKS_TOKEN', '')\n",
    "        print(\"Model context loaded successfully\")\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Process request with token limit checking\"\"\"\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            # Convert DataFrame to dict and get first row\n",
    "            if len(model_input) > 0:\n",
    "                data = model_input.iloc[0].to_dict()\n",
    "            else:\n",
    "                return {\"error\": \"Empty input DataFrame\"}\n",
    "        elif isinstance(model_input, dict):\n",
    "            data = model_input\n",
    "        else:\n",
    "            # Try to convert to dict\n",
    "            try:\n",
    "                data = dict(model_input)\n",
    "            except:\n",
    "                return {\"error\": f\"Unsupported input type: {type(model_input)}\"}\n",
    "        \n",
    "        # Extract and parse messages\n",
    "        messages = data.get(\"messages\", [])\n",
    "        if isinstance(messages, str):\n",
    "            try:\n",
    "                messages = json.loads(messages)\n",
    "            except json.JSONDecodeError:\n",
    "                return {\"error\": \"Invalid JSON in messages field\"}\n",
    "        \n",
    "        # Extract parameters with defaults\n",
    "        user_name = str(data.get(\"user_name\", \"test.user@databricks.com\"))\n",
    "        model_name = str(data.get(\"model\", \"gpt-4.1-2025-04-14\"))\n",
    "        \n",
    "        # Handle max_tokens in case missing, this is on request side, not the rate limiter\n",
    "        max_tokens_raw = data.get(\"max_tokens\", 128)\n",
    "        if pd.isna(max_tokens_raw) or max_tokens_raw is None:\n",
    "            max_tokens = 128\n",
    "        else:\n",
    "            max_tokens = int(max_tokens_raw)\n",
    "        \n",
    "        # Handle temperature in case missing\n",
    "        temperature_raw = data.get(\"temperature\", 0.7)\n",
    "        if pd.isna(temperature_raw) or temperature_raw is None:\n",
    "            temperature = 0.7\n",
    "        else:\n",
    "            temperature = float(temperature_raw)\n",
    "        \n",
    "        # Check current token usage\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT COALESCE(SUM(total_tokens), 0) as total_used\n",
    "            FROM token_usage \n",
    "            WHERE user_name = %s AND model_name = %s\n",
    "        \"\"\", (user_name, model_name))\n",
    "        \n",
    "        result = self.cursor.fetchone()\n",
    "        tokens_used = int(result[0]) if result and result[0] else 0\n",
    "        \n",
    "        # Check user's token limit\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT token_limit \n",
    "            FROM user_token_limits \n",
    "            WHERE user_name = %s AND model_name = %s\n",
    "        \"\"\", (user_name, model_name))\n",
    "        \n",
    "        limit_result = self.cursor.fetchone()\n",
    "        \n",
    "        if not limit_result:\n",
    "            return {\"error\": f\"No token limit found for user {user_name} and model {model_name}\"}\n",
    "        \n",
    "        token_limit = int(limit_result[0])\n",
    "        \n",
    "        # Check if limit exceeded\n",
    "        if tokens_used >= token_limit:\n",
    "            return {\n",
    "                \"error\": f\"Token limit exceeded. Used: {tokens_used}, Limit: {token_limit}\",\n",
    "                \"tokens_used\": tokens_used,\n",
    "                \"token_limit\": token_limit\n",
    "            }\n",
    "        \n",
    "        # Prepare request for FM endpoint\n",
    "        fm_request = {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        if self.api_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.api_token}\"\n",
    "        \n",
    "        try:\n",
    "            # Call FM endpoint\n",
    "            response = requests.post(\n",
    "                self.fm_endpoint,\n",
    "                json=fm_request,\n",
    "                headers=headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            fm_response = response.json()\n",
    "            \n",
    "            # Extract token usage from response\n",
    "            usage = fm_response.get(\"usage\", {})\n",
    "            prompt_tokens = int(usage.get(\"prompt_tokens\", 0))\n",
    "            completion_tokens = int(usage.get(\"completion_tokens\", 0))\n",
    "            total_tokens = int(usage.get(\"total_tokens\", 0))\n",
    "            \n",
    "            # Log token usage to database\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO token_usage (\n",
    "                    user_name, \n",
    "                    model_name, \n",
    "                    prompt_tokens, \n",
    "                    completion_tokens, \n",
    "                    total_tokens, \n",
    "                    request_timestamp,\n",
    "                    request_id,\n",
    "                    response_content\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                user_name,\n",
    "                model_name,\n",
    "                prompt_tokens,\n",
    "                completion_tokens,\n",
    "                total_tokens,\n",
    "                datetime.utcnow(),\n",
    "                fm_response.get(\"id\", \"\"),\n",
    "                json.dumps(fm_response)\n",
    "            ))\n",
    "            \n",
    "            # Add usage info to response\n",
    "            fm_response[\"usage_info\"] = {\n",
    "                \"tokens_used_total\": tokens_used + total_tokens,\n",
    "                \"token_limit\": token_limit,\n",
    "                \"tokens_remaining\": token_limit - (tokens_used + total_tokens)\n",
    "            }\n",
    "            \n",
    "            return fm_response\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\n",
    "                \"error\": f\"Failed to call FM endpoint: {str(e)}\",\n",
    "                \"tokens_used\": tokens_used,\n",
    "                \"token_limit\": token_limit\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Unexpected error: {str(e)}\",\n",
    "                \"tokens_used\": tokens_used,\n",
    "                \"token_limit\": token_limit\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4229cb4f-522e-442c-8939-5b767fea2fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test out Endpoint Locally, Log and Register to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062c3591-d15b-4c01-8cbe-fdda19bd7638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define signature - all fields required\n",
    "input_schema = Schema([\n",
    "    ColSpec(DataType.string, \"messages\"),\n",
    "    ColSpec(DataType.string, \"user_name\"),\n",
    "    ColSpec(DataType.string, \"model\"),\n",
    "    ColSpec(DataType.long, \"max_tokens\"),\n",
    "    ColSpec(DataType.double, \"temperature\")\n",
    "])\n",
    "\n",
    "output_schema = Schema([\n",
    "    ColSpec(DataType.string, \"response\")\n",
    "])\n",
    "\n",
    "signature = mlflow.models.ModelSignature(\n",
    "    inputs=input_schema,\n",
    "    outputs=output_schema\n",
    ")\n",
    "\n",
    "pip_requirements = [\n",
    "    \"mlflow\",\n",
    "    \"requests\",\n",
    "    \"psycopg2-binary\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "\n",
    "# Create test DataFrame (simulating what serving endpoint sends)\n",
    "test_df = pd.DataFrame([{\n",
    "    \"messages\": json.dumps([\n",
    "        {\"role\": \"user\", \"content\": \"Say 'Test Successful' and nothing else\"}\n",
    "    ]),\n",
    "    \"user_name\": \"test.user@databricks.com\",\n",
    "    \"model\": \"gpt-4.1-2025-04-14\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7\n",
    "}])\n",
    "\n",
    "print(\"Test input DataFrame:\")\n",
    "print(test_df)\n",
    "\n",
    "model = TokenLimitedGatewayModel()\n",
    "model.load_context(None)\n",
    "\n",
    "print(\"\\nTesting with DataFrame input...\")\n",
    "result = model.predict(None, test_df)\n",
    "if \"error\" not in result:\n",
    "    print(\"Test successful!\")\n",
    "    if \"choices\" in result:\n",
    "        print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    print(f\"Usage info: {result.get('usage_info', {})}\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"token_gateway\",\n",
    "        python_model=TokenLimitedGatewayModel(),\n",
    "        pip_requirements=pip_requirements,\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    model_uri = f\"runs:/{run.info.run_id}/token_gateway\"\n",
    "    print(f\"Model logged with URI: {model_uri}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "# Register to Unity Catalog\n",
    "catalog = \"<catalog_name>\"\n",
    "schema = \"<schema_name>\" \n",
    "model_name = \"token_limited_gateway\"\n",
    "\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=f\"{catalog}.{schema}.{model_name}\",\n",
    "    tags={\n",
    "        \"use_case\": \"rate_limiting\", \n",
    "        \"model_type\": \"gateway\",\n",
    "        \"backend\": \"openai_gpt4\",\n",
    "        \"database\": \"lakebase_postgres\",\n",
    "        \"version\": \"dataframe_compatible\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e5e8e7-c50e-4615-b5ba-2acd88cca8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4666361830765860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Rate Limiter Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}